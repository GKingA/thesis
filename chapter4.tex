%----------------------------------------------------------------------------
\chapter{Experiments and results}\label{sect:Experiments}
%----------------------------------------------------------------------------

In this chapter I will present my results achieved on the data. The two classes (whether to include a word or edge in the summary) appear disproportionally in the dataset. There is 2.5 times more node that we leave out from the summaries then nodes we take in and 4.8 times more edges left out than contained in the summary.

\section{Evaluation methods}

I used three methods to evaluate the results. The most basic method is the accuracy measure, which calculates the ratio of the correct answers as follows:

\[accuracy = \frac{correct\ answer}{all\ answer}\]

Since we are talking about graphs we need to measure the accuracy on the nodes and (if we are also training on them) the edges.
\[accuracy_{nodes\ and\ edges} = \frac{correct\ nodes + correct\ edges}{all\ nodes + all\ edges}\]

In our case this metric does not contain real information about the results of the model because of the high disparity between the classes.

The other general evaluation metric in case of classification is the F-score. This is better for evaluating the results of a classification because it is not so greatly affected by the distribution of the classes. For this we need to calculate the true positive, true negative, false positive and false negative measures for every class. See on Figue~\ref{fig:TP_TN_FP_FN}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=100mm, keepaspectratio]{figures/F_score.png}
	\caption{The visual representation of the concept of true positive, true negative, false positive and false negative measures.}
	\label{fig:TP_TN_FP_FN}
\end{figure}


\[TP_{node==1} = number\ of\ correctly\ guessed\ node==1\]
\[TN_{node==1} = number\ of\ correctly\ guessed\ node!=1\]
\[FP_{node==1} = number\ of\ incorrectly\ guessed\ node==1\]
\[FN_{node==1} = number\ of\ incorrectly\ guessed\ node!=1\]

With these measures we can calculate the precision and recall. The precision tells us the ratio of correct guesses of the class out of every time the model predicted that class. The recall is the ratio of the correct guesses out of every instance that are actually in the class. The harmonic mean of the precision and recall is the F-score.

\[precision = \frac{TP}{TP + FN}\]
\[recall = \frac{TP}{TP + FP}\]

\[F1 = 2 * \frac{precision * recall}{precision + recall}\]

I constructed extractive summaries from the resulting graphs and the original articles multiple ways described in section~\ref{ssect:ExperimnetsAndResults} and calculated the various ROUGE scores for each method's output.

\section{Experiments and results}\label{ssect:ExperimnetsAndResults}
I have experimented with both the Encode Process Decode and the Graph Attention Network models with varying success.
\subsection{Encode Process Decode}
At first I tried to get the Encode-Process-Decode model to predict whether to put something in the summary graph or not using only the word ids as features, but realized it would probably be more sufficient to also use the POS tags.

I also experimented with different loss functions, but for now i decided to use softmax cross entropy with output feature sizes of 2.

Training on more, than 7000 articles for 5 epochs (Early stopping stopped it from running any longer). I've got the following results.

\[Softmax\ cross\ entropy\ loss\ on\ nodes\ and\ edges\ averaged = 0.447\]

The classification report on the test set. These nodes and edges are the ones that were in the summary:

\[edges_{precision} = 0.0\]
\[edges_{recall} = 0.0\]
\[edges_{F1} = 0.0\]
\[nodes_{precision} = 62.8\]
\[nodes_{recall} = 21.3\]
\[nodes_{F1} = 31.9\]

As it is clear from this the network hardly ever classifies a node as "1", meaning that is should be in the summary graph, but when it does it is 62.8\% accurate.

This model has not been further evaluated.

\subsection{SimpleGraphAttention Network}
I trained the network on the whole training set (80\% of the whole dataset) for 3 epochs. It stopped after the third epoch because of the early stopping mechanism.

I evaluated the trained network on the test set and the results are the following:

\[Softmax\ cross\ entropy\ loss\ on\ nodes = 0.809\]
The classification report on the test set. These nodes and edges are the ones that were in the summary:
\[nodes_{precision} = 53.4\]
\[nodes_{recall} =54.2\]
\[nodes_{F1} = 53.8\]

\subsubsection{Summary reconstruction and evaluation}
I tried three different approaches at the summary extraction, each have been evaluated by the ROUGE scoring mechanism. For this I used the perl based pyrouge package.

\paragraph{Reconstruction with the node average}

This reconstruction method scores each sentence of the original article using the equation below where the \(score_{word}\) is the predicted probability of the word (node) being in the summary graph.

\[score_{sentence} = \frac{\sum_{word \in sentence\ and\ word \notin stopwords} score_{word}}{|word \in sentence\ and\ word \notin stopwords|}\]

Based on the calculated score of each sentence we can order them by their relevance and keep the four most relevant sentence from the article as the summary.

This method does not utilize the information of the graph structure nor does it prefer longer sentences. As a result the average number of words in a summary are 75.97.

For fair comparison I generated shorter TextRank based gensim summaries and cut down the number of sentences in the extracted summaries. I did this by comparing the number of words in each reconstructed summary and chose the number of sentences so the word counts would be as close as possible in each pair.

\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l |}
		\hline
		\textbf{Evaluation method}&\textbf{System}&\textbf{Gensim}&\textbf{Extracted}\\ \hline \hline
		\textbf{average word count}&75.97&77.01&74.14 \\ \hline
		\textbf{ROUGE-1}&39.06&41.77&59.11 \\ \hline
		\textbf{ROUGE-2}&12.56&14.08&29.95 \\ \hline
		\textbf{ROUGE-L}&24.03&25.99&40.66 \\ \hline
		\textbf{ROUGE-SU*}&15.60&17.34&33.73 \\ \hline
	\end{tabular}
	\caption{ROUGE scores on the test set with reconstruction method using only the node scores}
\end{table}
\FloatBarrier

\paragraph{Reconstruction with just the graph structure}

Contrary to the previous version this construction method does not use the output of the graph neural network but it is included here as contrast and as a step toward the next summarization method.

\begin{eqnarray*}
	score_{sentence} = \sum_{edge \in sentence\ graph} &[1\ if\ sender\ word \notin stopwords] + \\&[1\ if\ receiver\ word \notin stopwords]
\end{eqnarray*}

The ordering is based on this score. The summary contains the four best sentences. The average length of a summary is 138.42 word.

\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l |}
		\hline
		\textbf{Evaluation method}&\textbf{System}&\textbf{Gensim}&\textbf{Extracted}\\ \hline \hline
		
		\textbf{average word count}&138.42&135.59&103.51 \\ \hline
		\textbf{ROUGE-1}&54.17&56.35&73.91 \\ \hline
		\textbf{ROUGE-2}&18.82&20.80&39.29 \\ \hline
		\textbf{ROUGE-L}&32.68&34.47&51.01 \\ \hline
		\textbf{ROUGE-SU*}&26.69&29.27&49.15 \\ \hline
	\end{tabular}
	\caption{ROUGE scores on the test set with reconstruction method using just the graph structure}
\end{table}
\FloatBarrier

\paragraph{Reconstruction with the graph structure and the node scores}

This method is the blend of the previous two. It utilizes the structure of the graph but also uses the output of the graph neural network.
\begin{eqnarray*}
	score_{sentence} = \sum_{edge \in sentence\ graph} &[score_{sender\ word}\ if\ sender\ word \notin stopwords] + \\&[score_{receiver\ word}\ if\ receiver\ word \notin stopwords]
\end{eqnarray*}
These summaries also contain the best four sentences. The average number of words in these summaries are 127.11.
\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l |}
		\hline
		\textbf{Evaluation method}&\textbf{System}&\textbf{Gensim}&\textbf{Extracted}\\ \hline \hline
		\textbf{average word count}&127.11&135.59&103.51 \\ \hline
		\textbf{ROUGE-1}&56.80&56.35&73.91 \\ \hline
		\textbf{ROUGE-2}&21.97&20.80&39.29 \\ \hline
		\textbf{ROUGE-L}&34.95&34.47&51.01 \\ \hline
		\textbf{ROUGE-SU*}&29.45&29.27&49.15 \\ \hline
	\end{tabular}
\caption{ROUGE scores on the test set with reconstruction method using both graph structure and the node scores}
\end{table}

\subsubsection{Example}
To better visualize the results I compare the summary graph from Chapter~\ref{sect:DataProcessing} (Figure~\ref{fig:usain_summary_graph} and) to the calculated graph (Figure~\ref{fig:usain_bolt_predicted0}). The generated summaries are also written below.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=150mm, keepaspectratio]{figures/usain_bolt_predicted.png}
	\caption{The calculated summary graph of the example article with just the nodes labeled 1.}
	\label{fig:usain_bolt_predicted0}
\end{figure}

\FloatBarrier

\textbox{
	\textbf{Reconstruction with the node average}
	
	Fraser-Pryce, like Bolt aged 26, became the first woman to achieve three golds in the 100-200 and the relay.
	Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.
	The British quartet, who were initially fourth, were promoted to the bronze which eluded their men's team.
	Bolt's final dash for golden glory brought the eight-day championship to a rousing finale, but while the hosts topped the medal table from the United States there was criticism of the poor attendances in the Luzhniki Stadium.
	
	\textbf{Reconstruction with just the graph structure}
	
	The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles.
	Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple.
	The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.
	Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.
	
	\textbf{Reconstruction with the graph structure and the node scores}
	
	The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.
	Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.
	Germany's Christina Obergfoll finally took gold at global level in the women's javelin after five previous silvers, while Kenya's Asbel Kiprop easily won a tactical men's 1500m final.
	Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple.}{
	\caption{Example article summaries}}

\subsection{GraphAttention Network}

I trained this model on ten-thousand data for 3 epochs. In each epoch the network trained on different data, all in all I used thirty-thousand input and target output. It stopped after the third epoch because of the early stopping mechanism.

Due to time constraints I was unable to train the network on the whole training data.

I evaluated the trained network and the results are the following:

\[Softmax\ cross\ entropy\ loss\ on\ nodes = 0.7996\]
The classification report on the test set. These nodes and edges are the ones that were in the summary:
\[nodes_{precision} = 43.1\]
\[nodes_{recall} = 63.8\]
\[nodes_{F1} = 51.4\]

\subsubsection{Summary reconstruction and evaluation}
I tried the same reconstruction method for the results of the GraphAttention model. Keep in mind that this model has been trained for less time and on less data.

\paragraph{Reconstruction with the node average}

The average number of words in a four sentence long summary is 75.65.

\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l |}
	\hline
	\textbf{Evaluation method}&\textbf{System}&\textbf{Gensim}&\textbf{Extracted}\\ \hline \hline
	\textbf{average word count}&75.65&77.01&74.14 \\ \hline
	\textbf{ROUGE-1}&39.46&41.77&59.11 \\ \hline
	\textbf{ROUGE-2}&12.99&14.08&29.95 \\ \hline
	\textbf{ROUGE-L}&24.39&25.99&40.66 \\ \hline
	\textbf{ROUGE-SU*}&16.04&17.34&33.73 \\ \hline
	\end{tabular}
	\caption{ROUGE scores on the test set with reconstruction method using only the node scores}
\end{table}

\paragraph{Reconstruction with just the graph structure}

The result is the same as before, since we only consider the graph structures and they haven't changed.

\paragraph{Reconstruction with the graph structure and the node scores}

The average word count in a four sentence long summary is 130.21.

\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l |}
	\hline
		\textbf{Evaluation method}&\textbf{System}&\textbf{Gensim}&\textbf{Extracted}\\ \hline \hline
		\textbf{average word count}&130.21&135.59&103.51 \\ \hline
		\textbf{ROUGE-1}&56.02&56.35&73.91 \\ \hline
		\textbf{ROUGE-2}&21.03&20.80&39.29 \\ \hline
		\textbf{ROUGE-L}&34.28&34.47&51.01 \\ \hline
		\textbf{ROUGE-SU*}&28.61&29.27&49.15 \\ \hline
	\end{tabular}
	\caption{ROUGE scores on the test set with reconstruction method using both graph structure and the node scores}
\end{table}

\subsubsection{Example}
Similarly to the previous case I compared the summary graph from Chapter~\ref{sect:DataProcessing} (Figure~\ref{fig:usain_summary_graph} and) to the calculated graph (Figure~\ref{fig:usain_bolt_predicted1}). The generated summaries are also written below.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=150mm, keepaspectratio]{figures/usain_bolt_predicted_attended.png}
	\caption{The calculated summary graph of the example article with just the nodes labeled 1.}
	\label{fig:usain_bolt_predicted1}
\end{figure}

\FloatBarrier

\textbox{
	\textbf{Reconstruction with the node average}
	
	Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.
	Fraser-Pryce, like Bolt aged 26, became the first woman to achieve three golds in the 100-200 and the relay.
	The British quartet, who were initially fourth, were promoted to the bronze which eluded their men's team.
	The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.
	
	\textbf{Reconstruction with just the graph structure}
	
	The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles.
	Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple.
	The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.
	Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.
	
	\textbf{Reconstruction with the graph structure and the node scores}
	
	The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.
	Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover.
	Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple.
	Germany's Christina Obergfoll finally took gold at global level in the women's javelin after five previous silvers, while Kenya's Asbel Kiprop easily won a tactical men's 1500m final.}{}
\FloatBarrier

\section{Comparison between models}

Table~\ref{tab:short} contains the comparable ROUGE scores of the SimpleGraphAttention model's and the GraphAttention model's summaries reconstructed with the node average method and also the TextRank based benchmark method's output followed by the maximum achievable ROUGE scores with the adequate summary lengths.

\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		\textbf{Metric}&\textbf{Simple node avg}&\textbf{Attended node avg}&\textbf{Gensim}&\textbf{Extracted} \\ \hline \hline
		\textbf{avg words}&75.97&75.65&77.01&74.14 \\ \hline
		\textbf{ROUGE-1}&39.06&39.46&41.77&59.11 \\ \hline
		\textbf{ROUGE-2}&12.56&12.99&14.08&29.95 \\ \hline
		\textbf{ROUGE-L}&24.03&24.39&25.99&40.66 \\ \hline
		\textbf{ROUGE-SU*}&15.60&16.04&17.34&33.73 \\ \hline
	\end{tabular}
	\caption{Short summary comparisons}
	\label{tab:short}
\end{table}

Table~\ref{tab:long} contains the comparable ROUGE scores of the SimpleGraphAttention model's and the GraphAttention model's summaries reconstructed with the node scores and the graph structure and also the TextRank based benchmark method's output followed by the maximum achievable ROUGE scores all constructed by the top 4 sentences.

\begin{table}[!ht]
	\centering
	\begin{tabular}{| l | l | l | l | l | l |}
		\hline
		\textbf{Metric}&\textbf{Just structure}&\textbf{Simple}&\textbf{Attended}&\textbf{Gensim}&\textbf{Extracted} \\ \hline \hline
		\textbf{avg words}&138.42&127.11&130.21&135.59&103.51 \\ \hline
		\textbf{ROUGE-1}&54.17&56.80&56.02&56.35&73.91 \\ \hline
		\textbf{ROUGE-2}&18.82&21.97&21.03&20.80&39.29 \\ \hline
		\textbf{ROUGE-L}&32.68&34.95&34.28&34.47&51.01 \\ \hline
		\textbf{ROUGE-SU*}&26.69&29.45&28.61&29.27&49.15 \\ \hline
	\end{tabular}
	\caption{Long summary comparisons}
	\label{tab:long}
\end{table}
