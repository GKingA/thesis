%----------------------------------------------------------------------------
% Abstract in hungarian
%----------------------------------------------------------------------------
\chapter*{Kivonat}\addcontentsline{toc}{chapter}{Kivonat}

Diplomamunkám során annak a lehet\H{o}ségét vizsgálom, lehetséges-e  gráftranszformációként értelmezni a természetes nyelvfeldolgozásban kivonatolással létrehozott összefoglalásként ismert feladatot és gráf neurális hálót építeni ezen feladat megoldására a DeepMind graph\_nets könyvtárát felhasználva.

Kivonatolással létrehozott összefoglalás (angolul extractive summarization) alatt azt a feladatot értjük, amely során egy szöveghez összefoglalót képzünk a szövegben szerepl\H{o} modatok felhasználásával.

Munk\'am során megvalósítottam egy leképzést, mely cikkekb\H{o}l megfelel\H{o} universal dependency (UD) gráfot generál a stanfordnlp könyvtár felhasználásával. Mivel a gráfok tanulásakor feltétel, hogy az élek és csúcsok száma be és kimeneti gráfpáronként egyez\H{o}ek kell legyenek, így az egyes összefoglalókhoz képzett gráfokat ennek megfelel\H{o}en alakítottam ki.

A graph\_nets könyvtár tartalmaz egy Encode-Process-Decode modellt, amelyet kiindulási alapnak tudtam felhasználni munkám során. Több módszerrel is megvizsgáltam ezen modell használhatóságát a feladaton, majd az így szerzett tapaszalatokkal építettem két másik gráf neurális hálót, természetes nyelvfeldolgozás feladatok megoldására szabva.

A gráf neurális hálók tanítása kihívásokkal teli feladatnak bizonyult, mivel felépítése eltér a megszokott neurális háló struktúráktól. A hozzá kapcsolódó cikk és a demó példák segítették a megértést.

Az eredményeimet az adathalmazhoz tartozó szabad szavas összefoglalóhoz mértem, ezzel meghatározva a ROUGE pontját, valamint ezt összevetettem a kivonatolással elérhet\H{o} maximum ROUGE ponttal és a gensim könyvtár TextRank alapú összefoglalójával.

%----------------------------------------------------------------------------
% Abstract in english
%----------------------------------------------------------------------------
\chapter*{Abstract}\addcontentsline{toc}{chapter}{Abstract}

In my masters thesis I examined the possibility of using graph transformations for a natural language processing task known as extractive summarization and whether we could build a graph neural network for this purpose using DeepMind's graph\_nets library.

Extractive summarization is the task of generating a summary for a text using only the words, expressions and/or sentences from the original text.

I've used a standard method to transform the articles into their respective universal dependency (UD) graph using the stanfordnlp library. Since the structure of input and output graphs are required to be the same for the graph neural networks to be able to train on them I had to modify the graphs built from the summary accordingly.

The graph\_nets library already contains an Encode-Process-Decode model that proved to be a great starting point while exploring the task and possibilities. I experimented with the usability of this model on my task and with the observations I gathered I have built another graph neural network specifically for solving natural language processing tasks.

The training of these graphs was a challenging task, because their structure vastly differs from regular neural networks. The related article~\cite{GraphNet} and the demo examples helped me understand it better.

I compared the achieved results with the human-written summaries provided in the dataset determining the result's ROUGE score. This score was compared with the maximum achievable ROUGE score with extractive summarization and also with the summary generated by gensim's TextRank algorithm.